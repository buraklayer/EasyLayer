{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08874520-2ec5-4e04-80b0-b545038ebab4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# source = inspect.getsource(train)\n",
    "# parsed = ast.parse(source)\n",
    "\n",
    "# for node in ast.walk(parsed):\n",
    "#     if isinstance(node,ast.Call):\n",
    "#         if isinstance(node.func, ast.Attribute):\n",
    "#             if (node.func.value.id == 'layer'):\n",
    "#                     if(node.func.attr == 'get_dataset'):\n",
    "#                         print(ast.dump(node))\n",
    "#                         print(node.args[0].value)\n",
    "\n",
    "\n",
    "# ast.dump(parsed)\n",
    "\n",
    "import pandas as pd\n",
    "import inspect\n",
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "import ast\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    entities = []\n",
    "    entity_context = None\n",
    "\n",
    "    def __init__(self, project_name, environment):\n",
    "        self.project_name = project_name\n",
    "        self.environment = environment\n",
    "\n",
    "    def setup(self):\n",
    "        if os.path.exists(self.environment):\n",
    "            file1 = open(self.environment, 'r')\n",
    "            for lib in file1.readlines():\n",
    "                print(f\"Layer Infra: Installing {lib.strip()}...\")\n",
    "        else:\n",
    "            print(f\"Environment file not found: {self.environment}\")\n",
    "\n",
    "    def log_parameter(self, metric, value):\n",
    "        print(f\"\\t{Layer.entity_context} > Parameter > {metric}:{value}\")\n",
    "\n",
    "    def log_metric(self, metric, value):\n",
    "        print(f\"\\t{Layer.entity_context} > Metric >{metric}:{value}\")\n",
    "\n",
    "    def log(self, message):\n",
    "        print(f\"\\t{Layer.entity_context} > {message}\")\n",
    "\n",
    "    def run(self, entities):\n",
    "        self.entities = []\n",
    "        for entity in entities:\n",
    "            if entity._type == \"dataset\":\n",
    "                self.entities.append(Dataset(entity))\n",
    "            elif entity._type == \"model\":\n",
    "                self.entities.append(Model(entity))\n",
    "\n",
    "        print(f\"--- Layer Infra: Running Project: {self.project_name} ---\")\n",
    "\n",
    "        self.setup()\n",
    "\n",
    "        for entity in self.entities:\n",
    "            entity.run()\n",
    "        print(f\"\\n--- Layer Infra: Run Complete! ---\")\n",
    "\n",
    "    def get_dataset(self, name):\n",
    "        for entity in self.entities:\n",
    "            if entity.name == name:\n",
    "                return entity\n",
    "        raise Exception(f\"Entity '{name}' not found!\")\n",
    "\n",
    "\n",
    "class Model:\n",
    "    result = None\n",
    "\n",
    "    def __init__(self, func):\n",
    "        if func:\n",
    "            self.name = func._name\n",
    "            self.func = func\n",
    "\n",
    "    def run(self):\n",
    "        self.result = self.func()\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    result = None\n",
    "\n",
    "    def __init__(self, func):\n",
    "        if func:\n",
    "            self.name = func._name\n",
    "            self.func = func\n",
    "\n",
    "    def run(self):\n",
    "        self.result = self.func()\n",
    "\n",
    "    def to_pandas(self):\n",
    "        return self.result\n",
    "\n",
    "\n",
    "def dataset(name):\n",
    "    def inner(func):\n",
    "        func._type = \"dataset\"\n",
    "        func._name = name\n",
    "\n",
    "        def wrapped(*args):\n",
    "            Layer.entity_context = func._name\n",
    "            print(f'\\nBuilding {Layer.entity_context}...')\n",
    "            res = func()\n",
    "            # TODO save returning entity to catalog\n",
    "            return res\n",
    "        wrapped._type = \"dataset\"\n",
    "        wrapped._name = name\n",
    "\n",
    "        return wrapped\n",
    "\n",
    "    return inner\n",
    "\n",
    "\n",
    "def model(name):\n",
    "    def inner(func):\n",
    "        def wrapped(*args):\n",
    "            Layer.entity_context = name\n",
    "            print(f'\\nTraining {Layer.entity_context}...')\n",
    "            res = func()\n",
    "            # TODO save returning entity to catalog\n",
    "            return res\n",
    "        wrapped._type = \"model\"\n",
    "        wrapped._name = name\n",
    "\n",
    "        return wrapped\n",
    "\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7dc96d34-8516-4706-9233-2b079306b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Any\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "142d4e18-d9c6-40a7-9abd-b7a08888ddb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataset('orders_dataset')\n",
    "def read_orders_table() -> Any:\n",
    "    \n",
    "    # Read the csv file\n",
    "    orders_df = pd.read_csv(\"/Users/burakozen/Jupyter_Projects/data/olist_orders_dataset.csv\")\n",
    "    \n",
    "    # Converting object types to datetime for further analysis\n",
    "    orders_df.order_purchase_timestamp = pd.to_datetime(orders_df.order_purchase_timestamp)\n",
    "    orders_df.order_approved_at = pd.to_datetime(orders_df.order_approved_at)\n",
    "    orders_df.order_delivered_carrier_date = pd.to_datetime(orders_df.order_delivered_carrier_date)\n",
    "    orders_df.order_delivered_customer_date = pd.to_datetime(orders_df.order_delivered_customer_date)\n",
    "    orders_df.order_estimated_delivery_date = pd.to_datetime(orders_df.order_estimated_delivery_date)\n",
    "    \n",
    "    # Important Data Info \n",
    "    layer.log(f\"All order_ids are unique: {orders_df.customer_id.nunique()==len(orders_df)}\")\n",
    "    layer.log(f\"Max order purchase date: {orders_df.order_purchase_timestamp.max()}\")\n",
    "    layer.log(f\"Min order purchase date: {orders_df.order_purchase_timestamp.min()}\")\n",
    "    layer.log(f\"Different order statuses: {orders_df.order_status.unique()}\")\n",
    "    \n",
    "    return orders_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ddb1509-7f51-429d-875e-b6973df23ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataset('payments_dataset')\n",
    "def read_payments_table() -> Any:\n",
    "    \n",
    "    # Read the csv file\n",
    "    payments_df = pd.read_csv(\"/Users/burakozen/Jupyter_Projects/data/olist_order_payments_dataset.csv\")\n",
    "    \n",
    "    # Important Data Info\n",
    "    layer.log(f\"Total number of rows: {len(payments_df)}\")\n",
    "    layer.log(f\"Total number of unique orders: {payments_df.order_id.nunique()}\")\n",
    "    layer.log(f\"Payment Types: {payments_df.payment_type.unique()}\")\n",
    "    layer.log(f\"Payment Value Mean: {payments_df.payment_value.mean()}\")\n",
    "    \n",
    "    return payments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47e3f9e9-2b16-4563-b1de-0810c53eb85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataset('customers_dataset')\n",
    "def read_customers_table() -> Any:\n",
    "    \n",
    "    # Read the csv file\n",
    "    customers_df = pd.read_csv(\"/Users/burakozen/Jupyter_Projects/data/olist_customers_dataset.csv\")\n",
    "    \n",
    "    # Important Data Info\n",
    "    layer.log(f\"Total number of customers: {customers_df.customer_unique_id.nunique()}\")\n",
    "    layer.log(f\"Total number of different cities: {customers_df.customer_city.nunique()}\")\n",
    "    layer.log(f\"Total number of different states: {customers_df.customer_state.nunique()}\")\n",
    "    \n",
    "    return customers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cd354d-4df3-4b7c-a07a-111bc8955999",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataset('orders_general_features')\n",
    "def extract_orders_general_features() -> Any:\n",
    "    \n",
    "    # ORDER TABLE BASED FEATURE EXTRACTION - ORDER FEATURES\n",
    "    \n",
    "    # Get dataset from Layer and convert it into a pandas dataframe\n",
    "    orders_df = layer.get_dataset(\"orders_dataset\").to_pandas()\n",
    "    \n",
    "    # Drop rows which has NA values in the timestamp columns\n",
    "    orders_df = orders_df[orders_df['order_approved_at'].notna() \n",
    "                          & orders_df['order_purchase_timestamp'].notna() \n",
    "                          & orders_df['order_delivered_carrier_date'].notna() \n",
    "                          & orders_df['order_approved_at'].notna()\n",
    "                          & orders_df['order_estimated_delivery_date'].notna() \n",
    "                          & orders_df['order_delivered_customer_date'].notna()]\n",
    "    \n",
    "\n",
    "    # Data Sanity Check\n",
    "    orders_df = orders_df.loc[~((orders_df['order_approved_at'] < orders_df['order_purchase_timestamp']) | (orders_df['order_delivered_carrier_date'] < orders_df['order_approved_at'])),:]\n",
    "\n",
    "    # Computing new features: total_waiting & days_between_estimate_actual_delivery\n",
    "    orders_df[\"payment_approvement_waiting\"]=(orders_df.order_approved_at - orders_df.order_purchase_timestamp).dt.days\n",
    "    orders_df[\"delivered_carrier_waiting\"]=(orders_df.order_delivered_carrier_date - orders_df.order_approved_at).dt.days\n",
    "    orders_df[\"total_waiting\"] = orders_df.payment_approvement_waiting + orders_df.delivered_carrier_waiting\n",
    "    orders_df[\"days_between_estimate_actual_delivery\"]=(orders_df.order_estimated_delivery_date - orders_df.order_delivered_customer_date).dt.days\n",
    "\n",
    "    # Select features to be returned\n",
    "    orders_general_features=orders_df[['order_id','order_status','total_waiting','days_between_estimate_actual_delivery']]\n",
    "    \n",
    "    # Important Data Info\n",
    "    layer.log(f\"All total_waiting values non-negative: {(orders_general_features.total_waiting>=0).all()}\")\n",
    "    layer.log(f\"Max of days between estimate and actual delivery: {orders_general_features.days_between_estimate_actual_delivery.max()}\")\n",
    "    layer.log(f\"Min of days between estimate and actual delivery: {orders_general_features.days_between_estimate_actual_delivery.min()}\")\n",
    "\n",
    "    return orders_general_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081fb002-4840-4d87-b0a2-5638087a67e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataset('orders_payments_features')\n",
    "def extract_orders_payments_features() -> Any:\n",
    "    \n",
    "    # PAYMENT TABLE BASED FEATURE EXTRACTION - ORDER FEATURES\n",
    "    \n",
    "    # Fetch dataset from Layer and convert it into a pandas dataframe\n",
    "    payments_df = layer.get_dataset(\"payments_dataset\").to_pandas()\n",
    "    \n",
    "    # Drop rows which has NA values in the timestamp columns\n",
    "    payments_df = payments_df[payments_df['payment_type'].notna() & payments_df['payment_value'].notna()] \n",
    "\n",
    "    # Computing new features: use_voucher & total_payment & payment_type\n",
    "    payments_df_agg=payments_df\\\n",
    "    .assign(is_voucher= np.where(payments_df['payment_type']=='voucher',1,0))\\\n",
    "    .groupby(['order_id'],as_index=False) \\\n",
    "    .agg(use_voucher=(\"is_voucher\",\"max\"), \\\n",
    "         total_payment=(\"payment_value\",\"sum\"), \\\n",
    "         payment_type=(\"payment_type\",\"max\") \\\n",
    "        )\n",
    "\n",
    "    # Select columns to be returned\n",
    "    orders_payments_features=payments_df_agg[['order_id','use_voucher','total_payment','payment_type']]\n",
    "    \n",
    "    # Important Data Info\n",
    "    layer.log(f\"All total_payment values non-negative: {(orders_payments_features.total_payment>=0).all()}\")\n",
    "    \n",
    "    return orders_payments_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65d2e05a-5156-4e53-b41f-e5e0873f061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataset('customers_features')\n",
    "def extract_customer_features() -> Any:\n",
    "    \n",
    "    # CUSTOMER TABLE BASED FEATURE EXTRACTION - CUSTOMER FEATURES\n",
    "    \n",
    "    # Fetch datasets from Layer and convert them into a pandas dataframe\n",
    "    orders_df = layer.get_dataset(\"orders_dataset\").to_pandas()\n",
    "    customers_df = layer.get_dataset(\"customers_dataset\").to_pandas()\n",
    "    \n",
    "    #Merge 2 dataframes\n",
    "    orders_customers_merged = orders_df.merge(customers_df,left_on='customer_id',right_on='customer_id',how='left')\n",
    "\n",
    "    # # Compute a new feature: multiple_order\n",
    "    orders_customers_merged[\"total_orders\"]=orders_customers_merged.groupby('customer_unique_id')['customer_id'].transform('count')\n",
    "    orders_customers_merged['multiple_order'] = np.where(orders_customers_merged['total_orders']> 1, 1, 0)\n",
    "\n",
    "    #Compute a new feature: days_since_order\n",
    "    dataset_max_date=orders_customers_merged.order_purchase_timestamp.max()\n",
    "    orders_customers_merged['days_since_order'] = (dataset_max_date-orders_customers_merged.order_purchase_timestamp).dt.days\n",
    "\n",
    "    # Filter out only the first orders of users in the dataset\n",
    "    orders_customers_merged[\"order_rank\"]=orders_customers_merged.groupby('customer_unique_id')['order_purchase_timestamp'].rank(method='first')\n",
    "    orders_customers_merged=orders_customers_merged[orders_customers_merged[\"order_rank\"]==1.0].drop(columns=['order_rank'])\n",
    "\n",
    "    # Select columns to be returned and rename them accordingly\n",
    "    orders_customers_merged = orders_customers_merged.rename(columns={\"order_id\": \"first_order_id\", \"days_since_order\": \"days_since_first_order\"})\n",
    "    customer_features = orders_customers_merged[['customer_unique_id','customer_city','customer_state','first_order_id','days_since_first_order','multiple_order']]\n",
    "    \n",
    "    # Important Data Info\n",
    "    layer.log(f\"Ratio of multiple-orders customers over single-order customers: {round(customer_features.multiple_order.value_counts()[1]/customer_features.multiple_order.value_counts()[0],2)}\")\n",
    "\n",
    "    return customer_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41486a4a-36d3-43f9-a8af-b776168dc681",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataset('training_data')\n",
    "def generate_training_data() -> Any:\n",
    "    \n",
    "    # TRAINING DATA GENERATION\n",
    "    \n",
    "    # Fetch datasets from Layer and convert them into a pandas dataframe \n",
    "    orders_general_features = layer.get_dataset(\"orders_general_features\").to_pandas()\n",
    "    orders_payments_features = layer.get_dataset(\"orders_payments_features\").to_pandas()\n",
    "    customer_features = layer.get_dataset(\"customers_features\").to_pandas()\n",
    "    \n",
    "    \n",
    "    # Merge dataframes and drop irrelevant columns \n",
    "    order_features = orders_general_features.merge(orders_payments_features,left_on='order_id',right_on='order_id',how='left')\n",
    "    training_data_raw = customer_features.merge(order_features,left_on='first_order_id',right_on='order_id',how='left').drop(columns=['order_id','order_status'])\n",
    "\n",
    "    # Rename columns\n",
    "    training_data_raw = training_data_raw.rename(columns={\n",
    "                                      \"total_waiting\": \"first_order_total_waiting\", \n",
    "                                      \"days_between_estimate_actual_delivery\": \"first_order_days_between_estimate_actual_delivery\",\n",
    "                                      \"use_voucher\": \"first_order_use_voucher\",\n",
    "                                      \"total_payment\": \"first_order_total_payment\",\n",
    "                                      \"payment_type\": \"first_order_payment_type\"\n",
    "                                     })\n",
    "\n",
    "    # Decrease number of dimensions in the customer_city and customer_state columns to 6 (before applyling one-hot-encoding)\n",
    "    top5_cities = [\"sao paulo\",\"rio de janeiro\",\"belo horizonte\",\"brasilia\",\"curitiba\"]\n",
    "    top5_states = [\"SP\",\"RJ\",\"MG\",\"RS\",\"PR\"]\n",
    "    training_data_raw['customer_city'] = training_data_raw['customer_city'].apply(lambda city: city if city in top5_cities else 'other')\n",
    "    training_data_raw['customer_state'] = training_data_raw['customer_state'].apply(lambda state: state if state in top5_states else 'other')\n",
    "\n",
    "    # Create a label column 'churned': If multiple_order is 1, then CHURNED=0, otherwise CHURNED=1. Select only the churned customers if it has been more than 365 days since the first order\n",
    "    training_data_raw['churned'] = 1 \n",
    "    training_data_raw.loc[training_data_raw.multiple_order == 1, 'churned'] = 0\n",
    "    training_data_raw = training_data_raw.loc[ (training_data_raw.churned==0) | ((training_data_raw.churned==1) & (training_data_raw.days_since_first_order > 365))]\n",
    "\n",
    "    # Select columns to be returned and drop NA rows\n",
    "    training_data = training_data_raw.drop(columns=['multiple_order']).dropna()\n",
    "    \n",
    "    # Important Data Info\n",
    "    layer.log(f\"Number of training data records: {len(training_data)}\")\n",
    "    layer.log(f\"Churn user ratio after first order: {round(training_data.churned.value_counts()[1]/len(training_data),2)}\")\n",
    "    \n",
    "    return training_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "137894fb-5890-48fb-bffe-54bf11b740c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@model('churn_model')\n",
    "def train_churn_model() -> Any:\n",
    "    \n",
    "    # Fetch dataset from Layer and convert it into pandas dataframe\n",
    "    training_data = layer.get_dataset(\"training_data\").to_pandas()\n",
    "    \n",
    "    # Parameters for data split\n",
    "    test_size_fraction = 0.33\n",
    "    random_seed = 42   \n",
    "\n",
    "    # Data Split\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(training_data.drop(columns=['customer_unique_id','first_order_id','churned']),\n",
    "                                                        training_data.churned,\n",
    "                                                        test_size=test_size_fraction,\n",
    "                                                        random_state=random_seed)\n",
    "\n",
    "    \n",
    "    # Define a One-Hot Encoder Transformer\n",
    "    categorical_cols = ['customer_city','customer_state','first_order_payment_type']\n",
    "    transformer = ColumnTransformer(transformers=[('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)],remainder='passthrough')\n",
    "    \n",
    "    # Model: Define a Gradient Boosting Classifier\n",
    "    learning_rate = 0.01\n",
    "    max_depth = 6\n",
    "    max_features = 'sqrt'\n",
    "    min_samples_leaf = 10\n",
    "    n_estimators = 100\n",
    "    subsample = 0.8\n",
    "    random_state = 42\n",
    "    \n",
    "    layer.log_parameter(\"Test Size Fraction:\",test_size_fraction)\n",
    "    layer.log_parameter(\"Random Seed:\",random_seed)\n",
    "    layer.log_parameter(\"Learning Rate:\",learning_rate)\n",
    "    layer.log_parameter(\"Max Depth:\",max_depth)\n",
    "    layer.log_parameter(\"Max Number of Candidate Features:\",max_features)\n",
    "    layer.log_parameter(\"Min Number of Samples in a Leaf:\",min_samples_leaf)\n",
    "    layer.log_parameter(\"Number of Estimators:\",n_estimators)\n",
    "    layer.log_parameter(\"Subsample Ratio:\",subsample)\n",
    "    layer.log_parameter(\"Model's Random State\",random_state)\n",
    "    \n",
    "    model = GradientBoostingClassifier(learning_rate=learning_rate,\n",
    "                                       max_depth=max_depth,\n",
    "                                       max_features=max_features,\n",
    "                                       min_samples_leaf=min_samples_leaf,\n",
    "                                       n_estimators=n_estimators,\n",
    "                                       subsample=subsample,\n",
    "                                       random_state=random_state)\n",
    "    \n",
    "\n",
    "\n",
    "    # Pipeline Fit\n",
    "    pipeline = Pipeline(steps=[('t', transformer), ('m', model)])\n",
    "    pipeline.fit(X_train, Y_train)\n",
    "\n",
    "    # Model Evaluation\n",
    "    # 1. Predict probabilities of target 1:Churned\n",
    "    probs = pipeline.predict_proba(X_test)[:,1]\n",
    "    # 2. Calculate average precision and area under the receiver operating characteric curve (ROC AUC)\n",
    "    avg_precision = average_precision_score(Y_test, probs, pos_label=1)\n",
    "    auc = roc_auc_score(Y_test, probs)\n",
    "    \n",
    "    # Important Model Metrics\n",
    "    layer.log_metric(\"Average Precision Value:\",avg_precision)\n",
    "    layer.log_metric(\"Area under ROC:\",auc)\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd5429d7-ae44-42d7-a6d1-d8d5251c95b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Layer Infra: Running Project: churn_project ---\n",
      "Layer Infra: Installing scikit-learn>=0.18...\n",
      "\n",
      "Building orders_dataset...\n",
      "\torders_dataset > All order_ids are unique: True\n",
      "\torders_dataset > Max order purchase date: 2018-10-17 17:30:18\n",
      "\torders_dataset > Min order purchase date: 2016-09-04 21:15:19\n",
      "\torders_dataset > Different order statuses: ['delivered' 'invoiced' 'shipped' 'processing' 'unavailable' 'canceled'\n",
      " 'created' 'approved']\n",
      "\n",
      "Building payments_dataset...\n",
      "\tpayments_dataset > Total number of rows: 103886\n",
      "\tpayments_dataset > Total number of unique orders: 99440\n",
      "\tpayments_dataset > Payment Types: ['credit_card' 'boleto' 'voucher' 'debit_card' 'not_defined']\n",
      "\tpayments_dataset > Payment Value Mean: 154.10038041699553\n",
      "\n",
      "Building customers_dataset...\n",
      "\tcustomers_dataset > Total number of customers: 96096\n",
      "\tcustomers_dataset > Total number of different cities: 4119\n",
      "\tcustomers_dataset > Total number of different states: 27\n",
      "\n",
      "Building orders_general_features...\n",
      "\torders_general_features > All total_waiting values non-negative: True\n",
      "\torders_general_features > Max of days between estimate and actual delivery: 146\n",
      "\torders_general_features > Min of days between estimate and actual delivery: -189\n",
      "\n",
      "Building orders_payments_features...\n",
      "\torders_payments_features > All total_payment values non-negative: True\n",
      "\n",
      "Building customers_features...\n",
      "\tcustomers_features > Ratio of multiple-orders customers over single-order customers: 0.03\n",
      "\n",
      "Building training_data...\n",
      "\ttraining_data > Number of training data records: 29221\n",
      "\ttraining_data > Churn user ratio after first order: 0.9\n",
      "\n",
      "Training churn_model...\n",
      "\tchurn_model > Parameter > Test Size Fraction::0.33\n",
      "\tchurn_model > Parameter > Random Seed::42\n",
      "\tchurn_model > Parameter > Learning Rate::0.01\n",
      "\tchurn_model > Parameter > Max Depth::6\n",
      "\tchurn_model > Parameter > Max Number of Candidate Features::sqrt\n",
      "\tchurn_model > Parameter > Min Number of Samples in a Leaf::10\n",
      "\tchurn_model > Parameter > Number of Estimators::100\n",
      "\tchurn_model > Parameter > Subsample Ratio::0.8\n",
      "\tchurn_model > Parameter > Model's Random State:42\n",
      "\tchurn_model > Metric >Average Precision Value::0.9584324018084007\n",
      "\tchurn_model > Metric >Area under ROC::0.7898759194173129\n",
      "\n",
      "--- Layer Infra: Run Complete! ---\n"
     ]
    }
   ],
   "source": [
    "# ++ init Layer\n",
    "layer = Layer(project_name=\"churn_project\", environment='requirements.txt')\n",
    "\n",
    "# ++ To run the whole project on Layer Infra\n",
    "layer.run([read_orders_table, \n",
    "           read_payments_table, \n",
    "           read_customers_table,\n",
    "           extract_orders_general_features,\n",
    "           extract_orders_payments_features,\n",
    "           extract_customer_features,\n",
    "           generate_training_data,train_churn_model])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
